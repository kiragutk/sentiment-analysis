{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tokenizing\n",
    "Spliting the sentence into words (word_tokenize) the paragraph into lines (sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Programming', 'Language', 'has', 'a', 'great', 'future', '.']\n"
     ]
    }
   ],
   "source": [
    "example = 'Natural Programming Language has a great future.'\n",
    "print(word_tokenize(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Winter is coming.', 'I need a coat']\n"
     ]
    }
   ],
   "source": [
    "example = 'Winter is coming. I need a coat'\n",
    "print(sent_tokenize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Stop words\n",
    "Stopwords are the words which have no significant effect on the meaning of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "big\n",
      "fan\n",
      "motorsport\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "example = 'I am a big fan of the motorsport'\n",
    "words = word_tokenize(example)\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "         print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Part of Speed tagging\n",
    "We use part of speech tagging to tag the entity like a verb, adjective, noun, adverb, preposition etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Thank', 'NNP'), ('you', 'PRP'), ('very', 'RB'), ('much', 'RB'), ('.', '.'), ('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Mr.', 'NNP'), ('President', 'NNP'), (',', ','), ('distinguished', 'VBD'), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('honored', 'VBD'), ('guests', 'NNS'), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), ('.', '.'), ('May', 'NNP'), ('I', 'PRP'), ('congratulate', 'VBP'), ('all', 'DT'), ('of', 'IN'), ('you', 'PRP'), ('who', 'WP'), ('are', 'VBP'), ('members', 'NNS'), ('of', 'IN'), ('this', 'DT'), ('historic', 'JJ'), ('100th', 'JJ'), ('Congress', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('of', 'IN'), ('America', 'NNP'), ('.', '.'), ('In', 'IN'), ('this', 'DT'), ('200th', 'CD'), ('anniversary', 'JJ'), ('year', 'NN'), ('of', 'IN'), ('our', 'PRP$'), ('Constitution', 'NNP'), (',', ','), ('you', 'PRP'), ('and', 'CC'), ('I', 'PRP'), ('stand', 'VBP'), ('on', 'IN'), ('the', 'DT'), ('shoulders', 'NNS'), ('of', 'IN'), ('giants–men', 'NNS'), ('whose', 'WP$'), ('words', 'NNS'), ('and', 'CC'), ('deeds', 'NNS'), ('put', 'VBD'), ('wind', 'NN'), ('in', 'IN'), ('the', 'DT'), ('sails', 'NNS'), ('of', 'IN'), ('freedom', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "example = \"Thank you very much. Mr. Speaker, Mr. President, distinguished members of Congress, honored guests and fellow citizens. May I congratulate all of you who are members of this historic 100th           Congress of the United States of America. In this 200th anniversary year of our Constitution, you and I stand on the shoulders of giants–men whose words and deeds put wind in the sails of freedom.\"\n",
    "tagged = []\n",
    "words = []\n",
    "for lines in sent_tokenize(example):\n",
    "    for word in word_tokenize((lines)):\n",
    "        words.append(word)\n",
    "tagged.append(nltk.pos_tag(words))\n",
    "print(tagged)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Stemming\n",
    "Stripping down a word to its root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "example_words = ['python', 'pythoning', 'pythoned', 'pythoner']\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "# makes the program consume less memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Frequency Distribution\n",
    "To know the occurrence of a word in an article or most common x words in an article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('of', 8), ('.', 4), (',', 4), ('you', 3), ('and', 3)]\n"
     ]
    }
   ],
   "source": [
    "example = \"Thank you very much. Mr. Speaker, Mr. President, distinguished members of Congress, honored guests and fellow citizens. May I congratulate all of you who are members of this historic 100th Congress of the United States of America. In this 200th anniversary year of our Constitution, you and I stand on the shoulders of giants–men whose words and deeds put wind in the sails of freedom.\"\n",
    "words = [ ]\n",
    "for lines in sent_tokenize(example):\n",
    "    for word in word_tokenize((lines)):\n",
    "        words.append(word)\n",
    "words_dist = (nltk.FreqDist(words))\n",
    "print(words_dist.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Wordnet\n",
    "Wordnet is a huge collection of synsets, meanings, definition, examples, synonyms, antonyms etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('large.a.01'),\n",
       " Synset('big.s.02'),\n",
       " Synset('bad.s.02'),\n",
       " Synset('big.s.04'),\n",
       " Synset('big.s.05'),\n",
       " Synset('big.s.06'),\n",
       " Synset('boastful.s.01'),\n",
       " Synset('big.s.08'),\n",
       " Synset('adult.s.01'),\n",
       " Synset('big.s.10'),\n",
       " Synset('big.s.11'),\n",
       " Synset('big.s.12'),\n",
       " Synset('big.s.13'),\n",
       " Synset('big.r.01'),\n",
       " Synset('boastfully.r.01'),\n",
       " Synset('big.r.03'),\n",
       " Synset('big.r.04')]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Synsets\n",
    "words = wordnet.synsets('big')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "above average in size or number or quantity or magnitude or extent\n"
     ]
    }
   ],
   "source": [
    "#Definition\n",
    "print(words[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a large city', 'set out for the big city', 'a large sum', 'a big (or large) barn', 'a large family', 'big businesses', 'a big expenditure', 'a large number of newspapers', 'a big group of scientists', 'large areas of the world']\n"
     ]
    }
   ],
   "source": [
    "#Examples\n",
    "print(words[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonyms: {'bragging', 'great', 'bighearted', 'with_child', 'large', 'vainglorious', 'magnanimous', 'giving', 'braggart', 'handsome', 'swelled', 'bad', 'self-aggrandizing', 'freehanded', 'adult', 'heavy', 'gravid', 'vauntingly', 'openhanded', 'enceinte', 'fully_grown', 'grown', 'prominent', 'self-aggrandising', 'crowing', 'boastful', 'big', 'cock-a-hoop', 'boastfully', 'liberal', 'braggy', 'expectant', 'full-grown', 'grownup', 'bountiful', 'bounteous'}\n",
      "\n",
      "\n",
      "antonyms: {'small', 'little'}\n"
     ]
    }
   ],
   "source": [
    "#Synonyms and Antonyms\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "for words in wordnet.synsets('big'):\n",
    "    for word in words.lemmas():\n",
    "        synonyms.append(word.name())\n",
    "        if word.antonyms():\n",
    "            antonyms.append(word.antonyms()[0].name())\n",
    "print('synonyms: {}'.format(set(synonyms)))\n",
    "print('\\n')\n",
    "print('antonyms: {}'.format(set(antonyms)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data and Pickle\n",
    "We will load positive and negative movie reviews, split them into words and then process it in such a way that efficiency will increase. After the completion of the process we will pickle it so that we don’t need to process it again and again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are loading the datasets which consist of positive and negative reviews.\n",
    "neg_rev = open(r'C:\\Users\\Hp\\Desktop\\Data Analysis\\Sentiment Analysis\\negative.txt', 'rb').read()\n",
    "pos_rev = open(r'C:\\Users\\Hp\\Desktop\\Data Analysis\\Sentiment Analysis\\positive.txt', 'rb').read()\n",
    "\n",
    "# First of all, we split the data by a new line using splitlines() then save in pos and neg.\n",
    "pos = []\n",
    "neg = []\n",
    "for rev in pos_rev.splitlines():\n",
    "    pos.append(rev)\n",
    "for rev in neg_rev.splitlines():\n",
    "    neg.append(rev)\n",
    "    \n",
    "# Convert the lines into words using word_tokenize() function and save these words in pos_words and neg_words.\n",
    "pos_words = []\n",
    "neg_words = []\n",
    "\n",
    "for pos_line in pos:\n",
    "    pos_words.append(word_tokenize(str(pos_line)))\n",
    "for neg_line in neg:\n",
    "    neg_words.append(word_tokenize(str(pos_line)))\n",
    "\n",
    "# Since the words are arranged in a list of list, then first we iterate to each list or line \n",
    "# and then each word in that line and save the words in pos_word_new and neg_word_new.\n",
    "pos_words_new = []\n",
    "neg_words_new = []\n",
    "\n",
    "for line in pos_words:\n",
    "    for words in line:\n",
    "        pos_words_new.append(words)\n",
    "        \n",
    "for line in neg_words:\n",
    "    for words in line:\n",
    "        neg_words_new.append(words)\n",
    "\n",
    "# Now we have all the words that are in the datasets but the number is huge and most of the words are stopwords\n",
    "# Remove stopwords and save this new list of refined words in pos_words_new_stopwords and neg_words_new_stopwords.\n",
    "pos_words_new_stopwords = []\n",
    "neg_words_new_stopwords = []\n",
    "\n",
    "for words in pos_words_new:\n",
    "    if words not in stop_words:\n",
    "        pos_words_new_stopwords.append(words)\n",
    "        \n",
    "for words in neg_words_new:\n",
    "    if words not in stop_words:\n",
    "        neg_words_new_stopwords.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment we have a refined list of words which belong to positive and negative reviews but still, there are some words like actors names, car names and all sort of words which are redundant for us. So, in this step, we are tagging the words.\n",
    "We are only interested in Adjective(JJ), because they are the main words which show the positivity or negativity of a sentence, like awesome, worst, good, bad etc. then save them to pos_adj and neg_adj."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_pos = []\n",
    "tagged_neg = []\n",
    "pos_adj = []\n",
    "neg_adj = []\n",
    "\n",
    "tagged_pos.append(nltk.pos_tag(pos_words_new_stopwords))\n",
    "for i in range(len(tagged_pos[0])):\n",
    "    if tagged_pos[0][i][1] == 'JJ':\n",
    "        try:\n",
    "            pos_adj.append((tagged_pos[0][i][1]))\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "tagged_neg.append(nltk.pos_tag(neg_words_new_stopwords))\n",
    "for i in range(len(tagged_neg[0])):\n",
    "    if tagged_neg[0][i][1] == 'JJ':\n",
    "        neg_adj.append((tagged_neg[0][i][1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, there are many words which are in both positive set and negative set, but the number of occurrence plays an important role. For example:\n",
    "\n",
    "1. This movie is good\n",
    "2. This movie is not good,\n",
    "\n",
    "One is positive but another is negative but 'good' is in both the sets. If we take a singular instance of each word for each set then “good” become null and void, so to counter this, if any word is in the positive set and in the negative set then we remove it in both the sets.\n",
    "\n",
    "For example, if there are 30 “good” in positive set but 5 “good” in negative set then we will end up with 25 “good” in positive set and 0 “good” in negative set. This will reduce words in our data set and increase the speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pos_adj:\n",
    "    for i in neg_adj:\n",
    "        pos_adj.remove(i)\n",
    "        neg_adj.remove(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now find the synonyms for each word in positive data set and append these synonyms words in a new list pos_syn and neg_syn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_syn = []\n",
    "neg_syn = []\n",
    "\n",
    "for words in pos_adj:\n",
    "    for syn in wordnet.synsets(words):\n",
    "        for syn_word in syn.lemmas():\n",
    "            pos_syn.append(syn_word.name)\n",
    "            \n",
    "for words in neg_adj:\n",
    "    for syn in wordnet.synsets(words):\n",
    "        for syn_word in syn.lemmas():\n",
    "            neg_syn.append(syn_word.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we are using the set() function. This function reduces any number of instance of a word to one. Like if the data contains 401 words out of which 400 “good” word and 1 “bad” word then after this operation, the dataset will contain only 2 words; “good” and “bad”.  \n",
    "\n",
    "Note that we are only applying the set() function on synonym words. We are also converting set dataset into a list using type casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_syn = list(set(pos_syn))\n",
    "neg_syn = list(set(neg_syn))\n",
    "\n",
    "for words in pos_adj:\n",
    "    pos_syn.append(words)\n",
    "for words in neg_adj:\n",
    "    neg_syn.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we are using the FreqDist() function from NLTK. It returns the word and number of occurrence of that word in the list.\n",
    "By using dict typecasting, we will get a dictionary in which the key is the word and value is the number of occurrences.\n",
    "We need the number of occurrences to increase the weight of a word in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_adj_FreqDist = dict(nltk.FreqDist(pos_syn))\n",
    "neg_adj_FreqDist = dict(nltk.FreqDist(neg_syn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now search for the same words in both the dictionary and if found then we delete it and adjust the values, just like we did in cell 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {}\n",
    "neg_dict = {}\n",
    "count = 0\n",
    "\n",
    "for key1, value1 in pos_adj_FreqDist.items():\n",
    "    for key2, value2 in neg_adj_FreqDist:\n",
    "        if key1 == key2:\n",
    "            count += 1\n",
    "            if(value1 > value2):\n",
    "                value1 = value1 - value2\n",
    "                value2 = 0\n",
    "                pos_dict.update({key1:value1})\n",
    "            elif (value2 > value1):\n",
    "                value2 = value2 - value1\n",
    "                value1 = 0\n",
    "                neg_dict.update({key2:value2})\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are many words which may be a name, or address, or anything else which don’t contribute any part in the analysis. So for this, we are again tagging each word with its part of speech, and extracting the adjective as we did earlier just to double check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we assign a part of speech tag to the keys of positive & negative dictionry and save them into a list.\n",
    "tagged_neg_dict = []\n",
    "tagged_pos_dict = []\n",
    "tagged_neg_dict_list = []\n",
    "tagged_pos_dict_list = []\n",
    "\n",
    "tagged_pos_dict.append(nltk.pos_tag(pos_dict.keys()))\n",
    "for i in range(len(tagged_pos_dict[0])):\n",
    "    if tagged_neg_dict_list[0][i][1]==\"JJ\":\n",
    "        tagged_pos_dict_list.append(tagged_pos_dict[0][i][0])\n",
    "                                                       \n",
    "tagged_neg_dict.append(nltk.pos_tag(neg_dict.keys()))\n",
    "for i in range(len(tagged_neg_dict[0])):\n",
    "    if tagged_neg_dict[0][i][1]==\"JJ\":\n",
    "        tagged_neg_dict_list.append(tagged_neg_dict[0][i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of adjectives, in this step, we are making a combination of these adjectives and number of occurrence.\n",
    "Since we have pos_dict and neg_dict in which there are all the words and values, we will now find these adjectives in this list and if found then copy the values and making a new and final dictionary.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict_updated = []\n",
    "neg_dict_updated = []\n",
    "\n",
    "for key1, value1 in pos_dict.items():\n",
    "    for i in range(len(tagged_pos_dict_list)):\n",
    "        if key1 == tagged_pos_dict_list[1]:\n",
    "            pos_dict_updated.update({key1:value1})\n",
    "\n",
    "for key1, value1 in neg_dict.items():\n",
    "    for i in range(len(tagged_neg_dict_list)):\n",
    "        if key1 == tagged_neg_dict_list[1]:\n",
    "            neg_dict_updated.update({key1:value1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have a dictionary in which key is the adjective words and value is the number of occurrences. In this step, we are pickling these dictionaries so that we don’t have to make them again and again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(r'C:\\Users\\Hp\\Desktop\\Data Analysis\\Sentiment Analysis\\pos-yy_adj.pickle', 'wb')\n",
    "pickle.dump(pos_dict_updated, pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open(r'C:\\Users\\Hp\\Desktop\\Data Analysis\\Sentiment Analysis\\neg-yy_adj.pickle', 'wb')\n",
    "pickle.dump(neg_dict_updated, pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Predicting Function and testing it.\n",
    "Create a function to predict the nature of sentences. Our approach will be to split the words in the sentence and then count the number of occurrence of these words. If the sentence contains more positive words then it’s a positive type, else it will consider as negative type,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our pickles which we saved previously and then saving to a variable called pos_adj and neg_adj.\n",
    "pickle_out = open(r'C:\\Users\\Hp\\Desktop\\Data Analysis\\Sentiment Analysis\\pos-yy_adj.pickle', 'rb')\n",
    "pos_adj = pickle.load(pickle_out)\n",
    "pickle_out = open(r'C:\\Users\\Hp\\Desktop\\Data Analysis\\Sentiment Analysis\\neg-yy_adj.pickle', 'rb')\n",
    "neg_adj = pickle.load(pickle_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating a function which we will use to predict the nature of tweets, we call this function “check” then we initialize counter variables, which we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(example):\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    \n",
    "# spliting the lines into words. and saving them in ex_words.\n",
    "    ex_words = word_tokenize(example)\n",
    "\n",
    "# Iterating each word in the list of ex_words and then search if the word is in stop words or not.\n",
    "# If not we search the occurrence of the word in our pos_dict key set, if found we add the value of occurrence with pos_count. \n",
    "# Similarly, we search for the word in the negative set and add the value with neg_count.\n",
    "    global count\n",
    "    for ex_word in ex_words:\n",
    "        if ex_word.lower() not in stop_words:\n",
    "            for key, value in pos_dict.items():\n",
    "                if key == ex_word.lower():\n",
    "                    pos_count += value\n",
    "            for key, value in neg_dict.items():\n",
    "                if key == ex_word.lower():\n",
    "                    neg_count += value\n",
    "    \n",
    "    if pos_count>neg_count:\n",
    "        conf=pos_count-neg_count\n",
    "        checker=\"pos\"\n",
    "                     \n",
    "    elif pos_count<neg_count:\n",
    "        conf=neg_count-pos_count\n",
    "        checker=\"neg\"\n",
    "        \n",
    "    elif pos_count==neg_count:\n",
    "        checker=\"None\"\n",
    "        conf=0\n",
    "        \n",
    "# In the end, we return two variables, checker & conf. \n",
    "# checker is pos, neg or none, while conf is difference between pos_count & neg_count.\n",
    "# In case of None, the conf becomes zero.\n",
    "    return checker, conf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ABOVE: if the value of pos_count is greater then the neg_count then we can conclude that sentence has more positive words thus the sentence is positive type similarly we can check for negative.\n",
    "We also calculate the difference between both the counters and save it in confidence or \"conf\" variable. Higher the conf value, higher the accuracy, We can use it to add another layer of reliability.\n",
    "\n",
    "Now if the pos_count and neg_count become equal then we conclude that the sentence is neutral or doesn’t have words which matches with our pos and neg dictionary so we toss them out and mark as ‘None’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('None', 0)\n",
      "('None', 0)\n",
      "('None', 0)\n"
     ]
    }
   ],
   "source": [
    "# Here we have three examples. Lets check the prediction values.\n",
    "example_1 = \"The movie is just a waste of time, it's complete junk, Totally waste of money.\"\n",
    "example_2 = \"The food of this restaurant is very good, I will recommend this place to everyone.\"\n",
    "example_3 = \"This is a low-quality product, even the reviews of this product is very poor.\"\n",
    "\n",
    "print(predict(example_1))\n",
    "print(predict(example_2))\n",
    "print(predict(example_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tweets from Twitter using Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From predict function we tested we have three labels which we can assign to a tweet, if the tweet has more positive words, we will assign “pos”, if the tweet has more negative word then we assign “neg” and if in case the tweet contains both positive and negative value in equal amount then we  assign “None” label to the particular tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(example):\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    \n",
    "    ex_words = word_tokenize(example)\n",
    "\n",
    "    for ex_word in ex_words:\n",
    "        if ex_word.lower() not in stop_words:\n",
    "            for key, value in pos_dict.items():\n",
    "                if key == ex_word.lower():\n",
    "                    pos_count += value\n",
    "            for key, value in neg_dict.items():\n",
    "                if key == ex_word.lower():\n",
    "                    neg_count += value\n",
    "    \n",
    "    if pos_count>neg_count:\n",
    "        conf=pos_count-neg_count\n",
    "        checker=\"pos\"\n",
    "                     \n",
    "    elif pos_count<neg_count:\n",
    "        conf=neg_count-pos_count\n",
    "        checker=\"neg\"\n",
    "        \n",
    "    elif pos_count==neg_count:\n",
    "        checker=\"None\"\n",
    "        conf=0\n",
    "    \n",
    "    return checker, conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After assigning the labels,  we open a file and save these labels in it. We will use this file to read the labels and plot the graph in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove(r'C:\\Users\\Hp\\Desktop\\Data Analysis\\Sentiment Analysis\\test-yy_twitter.txt')    \n",
    "except:\n",
    "    output = open(r'C:\\Users\\Hp\\Desktop\\Data Analysis\\Sentiment Analysis\\test-yy_twitter.txt',\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a class which we will use to load the tweets, now there are two functions is the class. First one is on_data which we will use to defining the tweets variable and doing analysis, second is on_error which we will use if we encounter any error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class listener(StreamListener):\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        tweets = json.loads(data)\n",
    "        tweet = tweets['text']\n",
    "        created_date = tweets['created_at']\n",
    "        label, con = check(tweet)\n",
    "        output = open(r'C:\\Users\\Hp\\Desktop\\Data Analysis\\Sentiment Analysis\\test-yy_twitter.txt',\"a\")\n",
    "        output.write(label)\n",
    "        output.write('\\n')\n",
    "        output.close()\n",
    "        return True\n",
    "    \n",
    "    def on_error(self, status):\n",
    "        if status == '420':\n",
    "            print(status)\n",
    "            print('Multiple Connections, Try again after sometime')\n",
    "        else:\n",
    "            print(status)\n",
    "        \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "twitterStream = Stream(auth, listener())\n",
    "twitterStream.filter(track = ['movie'])\n",
    "output.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
